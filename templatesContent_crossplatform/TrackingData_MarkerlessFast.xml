<?xml version="1.0"?>
<!--	Sample tracking configuration for using the "fast" markerless tracker.
		The tracking does "localization" and then "interframe tracking".
		This kind of sensor tracks planar objects (printed images, posters,
		etc.) by matching their intensities. The "fast" variant has the
		following properties:
		- It works at higher framerates.
		- It is able to track multiple planar objects simultaneously.
		- It can be sensitive to occlusions, specularities and light changes.
		Unless otherwise noted, all the values used in this configuration are
		the default values.
		-->

<TrackingData>
	<Sensors>
		<!--	Use the "Fast" variant of the "FeatureBasedSensorSource". The
				characteristics of this tracker are explained above. -->
		<Sensor Type="FeatureBasedSensorSource" Subtype="Robust">

			<!--	Assign an ID to this sensor -->
			<SensorID>FeatureTracking1</SensorID>

			<!--	Parameters that apply to the sensor -->
			<Parameters>

				<!--	The following feature descriptor types are available:
						"regular", "upright", "gravity", "rectified".
						- The "regular" feature descriptor type is the most
						  general feature descriptor type and is used as
						  default if the tag is not specified.
						- The "upright" feature descriptor type assumes that
						  the camera is not rotated with respect to the optical
						  axis, i.e. is turned upside down, during the tracking
						  process.
						- The "gravity" feature descriptor type can only be
						  used with devices with inertial sensors which
						  measures gravity. It is used for localizing static
						  objects that provide (close to) vertical surfaces,
						  e.g. buildings or posters on a wall. The orientation
						  of the features will then be aligned with gravity.
						- The "rectified" feature descriptor type can only be
						  used with devices with inertial sensors which
						  measures gravity. It is used for planar objects on a
						  horizontal surface, e.g. a magazine on a table.
						  This will improve the result of the localization of
						  planar objects under steep camera angles at the cost
						  of a lower framerate during localization.
						This parameter is for expert usage only. In general it
						is advised to leave the value unchanged. -->
				<FeatureDescriptorAlignment>regular</FeatureDescriptorAlignment>

				<!--	A restriction on the number of reference planar objects
						to be localized per frame. Localization takes longer
						than interframe tracking, and if the system tries to
						localize too many objects at the same time, it might
						cause a lower framerate. The default value for this is 5
						and is used if the tag is not specified.
						Another name that can be used for this parameter is
						<MultipleReferenceImagesFast>. This name is however
						deprecated and should not be used any more.
						This parameter is for expert usage only. In general it
						is advised to leave the value unchanged. -->
				<MaxObjectsToDetectPerFrame>1</MaxObjectsToDetectPerFrame>

				<!--	The maximum number of objects that should be tracked in
						parallel. Tracking many objects in parallel is quite
						expensive and might lead to a lower framerate. As soon
						as the maximum number of tracked objects is reached,
						the system will no longer try to localize new objects.
						The default value for this is 1 and is used if the tag
						is not specified.
						Another name that can be used for this parameter is
						<MaxNumCosesForInit>. This name is however deprecated
						and should not be used any more.
						This parameter is for expert usage only. In general it
						is advised to leave the value unchanged. -->
				<MaxObjectsToTrackInParallel>1</MaxObjectsToTrackInParallel>

				<!--	Default similarity threshold for specifying whether
						template tracking was successful or failed. The
						tracking quality measure is defined between -1 and 1,
						where 1 is the best	possible value. If the tracking
						quality	is reported to be below the threshold, the
						tracker will treat the corresponding frame as lost.
						The default value for this is 0.7 and is used if the
						tag is not specified. This setting can be overridden
						for each "COS" if it is defined there.
						This parameter is for expert usage only. In general it
						is advised to leave the value unchanged. -->
				<SimilarityThreshold>0.7</SimilarityThreshold>

			</Parameters>

			<!--	Define a "SensorCOS" for this sensor. This is essentially a
					coordinate system associated with a template image that is
					to be tracked. -->
			<SensorCOS>

				<!--	An ID that this COS is associated with. -->
				<SensorCosID>Patch1</SensorCosID>

				<!--	Parameters that should be applied to this "SensorCOS".
						-->
				<Parameters>

					<!--	Reference image file name. If available, width and
							height of the tracking template in millimeters can
							be specified by adding attributes "WidthMM" and/or
							"HeightMM". If these values are not specified at
							all, the width and height of the reference image in
							pixels will be used. If only width or heigth are
							specified, the missing quantity will be calculated
							such that sizes agree with the aspect ratio of the
							image.
							These parameters are not required and are intended
							for expert use only. -->
					<ReferenceImage>altro.jpg</ReferenceImage>
					<!-- <ReferenceImage WidthMM="100" HeightMM="100">metaioman_target.png</ReferenceImage> -->

					<!--	Another similarity threshold can be specified here.
							It will override the default similarity threshold
							specified for the sensor.
							This parameter is for expert usage only. In general
							it is advised to leave the value unchanged. -->
					<SimilarityThreshold>0.7</SimilarityThreshold>

				</Parameters>
			</SensorCOS>

			<!--	The commented lines below show how another COS can be
					added to the configuration. -->
			<!--
			<SensorCOS>
				<SensorCosID>Patch2</SensorCosID>
				<Parameters>
					<ReferenceImage>junaioman.png</ReferenceImage>
					<SimilarityThreshold>0.8</SimilarityThreshold>
				</Parameters>
			</SensorCOS>
			-->

		</Sensor>
       <!--
        <Sensor type="BarCodeSensorSource">
            <SensorID>BarCodeSensor1</SensorID>
            <Parameters>
                <BarCodeShowRecognizedPoints />
                <BarCodeUseCropBox />
                <UsedFormats>
                    <Format>QR_CODE</Format>
                </UsedFormats>
            </Parameters>
            <SensorCOS>
                <SensorCosID>id1</SensorCosID>
            </SensorCOS>
        </Sensor>
	</Sensors> -->

	<!--	Connections between SensorCOS and COS entities are defined here.
			While the SensorCOS represents the pose of the tracked object
			relative to the sensor, the COS is the pose that will be used when
			augmenting objects. The COS is computed from the SensorCOS by
			performing additional processing steps:
			- A fuser can be used to smooth motion, and also to predict motion
			  in case of missing sensor readings.
			- A rigid transformation can be applied. The model to be augmented
			  can be shifted and rotated against a SensorCOS.
			- A hand-eye calibration can be applied.
			-->
	<Connections>
		<COS>

			<!--	A descriptive name for this COS. -->
			<Name>MarkerlessCOS1</Name>

			<!--	Which type of Fuser to use. Here, we use the
					"SmoothingFuser", which applies smoothing in order to predict
					movement and handle noise.
					-->
			<Fuser Type="SmoothingFuser">
				<Parameters>

					<!--	Number of frames in which the tracker will continue
							predicting the pose when interframe tracking
							fails. After the specified number of frames, the
							tracker will stop predicting.
							This parameter is for expert usage only. In general
							it is advised to leave the value unchanged. -->
					<KeepPoseForNumberOfFrames>2</KeepPoseForNumberOfFrames>

					<!--	If the tracking device is equipped with an inertial
							sensor that can measure gravity, the sensor's
							measurement is used to improve the pose
							estimate. To activate this option, the value of
							this tag must be set to "ReplaceZAxis".
							This parameter is for expert usage only. In general
							it is advised to leave the value unchanged. -->
					<GravityAssistance></GravityAssistance>

					<!--	Data (position) smoothing factor for double
							exponential smoothing of translation. This value
							should be high if measures are expected to be
							accurate and low otherwise. A high value assigns a
							higher weight to a new measurement. Typically, the
							accuracy of translation estimates is rather
							high, so we set the smoothing factor to 0.8. The
							default value is 0.5.
							This parameter is for expert usage only. In general
							it is advised to leave the value unchanged. -->
					<AlphaTranslation>1.0</AlphaTranslation>

					<!--	Trend (velocity) smoothing factor for double
							exponential smoothing of translation. This value
							should be high if measures are expected to be
							accurate and low otherwise. With the same
							reasoning as above, we set the smoothing factor to
							0.8. The default value is 0.5.
							This parameter is for expert usage only. In general
							it is advised to leave the value unchanged. -->
					<GammaTranslation>1.0</GammaTranslation>

					<!--	Data (position) smoothing factor for double
							exponential smoothing of rotation. Rotation
							measurements are typically not as accurate as
							translation measurements, so we use a value of 0.5.
							This parameter is for expert usage only. In general
							it is advised to leave the value unchanged. -->
					<AlphaRotation>0.8</AlphaRotation>

					<!--	Trend (velocity) smoothing factor for double
							exponential smoothing of rotation. With the same
							reasoning as for AlphaRotation above, we set this
							value to 0.5.
							This parameter is for expert usage only. In general
							it is advised to leave the value unchanged. -->
					<GammaRotation>0.8</GammaRotation>

					<!--	If an orientation sensor is available, the system
							may try to keep updating the pose based on that
							orientation sensor's measurements. If this should
							be done, then this option must be set to true. The
							default value is false.
							This parameter is for expert usage only. In general
							it is advised to leave the value unchanged. -->
					<ContinueLostTrackingWithOrientationSensor>true</ContinueLostTrackingWithOrientationSensor>
				</Parameters>
			</Fuser>

			<SensorSource>

				<!--	The Sensor and SensorCOS that we want to use. Note
						that these IDs are the same that we have used in the
						Sensor definition above. -->
				<SensorID>FeatureTracking1</SensorID>
				<SensorCosID>Patch1</SensorCosID>

				<!--	A hand-eye calibration allows to specify the relative
						pose between two sensors. In the simple case of having
						one camera-based sensor, it is usually not used. It
						allows to move the COS "as if" the camera were moved,
						and is thus inverse to the COSOffset rigid
						transformation that is specified below. -->
				<HandEyeCalibration>

					<!--	The 3D translation vector. -->
					<TranslationOffset>
						<X>0</X>
						<Y>0</Y>
						<Z>0</Z>
					</TranslationOffset>

					<!--	Rotations are specified via unit quaternions, where
							the imaginary parts "X", "Y", "Z" is specified
							first, and then the real part "W". -->
					<RotationOffset>
						<X>0</X>
						<Y>0</Y>
						<Z>0</Z>
						<W>1</W>
					</RotationOffset>
				</HandEyeCalibration>

				<!--	The COSOffset specifies a rigid transformation that
						is applied to the SensorCOS. This makes it possible to
						move the augmented model. It is specified just the same
						way as the hand-eye-calibration. -->
				<COSOffset>
					<TranslationOffset>
						<X>0</X>
						<Y>0</Y>
						<Z>0</Z>
					</TranslationOffset>
					<RotationOffset>
						<X>0</X>
						<Y>0</Y>
						<Z>0</Z>
						<W>1</W>
					</RotationOffset>
				</COSOffset>
			</SensorSource>
		</COS>

		<!--	The commented lines below show how another COS can be added to
				the configuration. This can be used together with the
				commented-out SensorCOS part in the Sensor definition above to
				create another COS. Note however that the robust tracker cannot
				track multiple objects in parallel, it will always only track
				one of the defined objects at the same time.
				-->

		<COS>
			<Name>MarkerlessCOS2</Name>
			<Fuser Type="BestQualityFuser">
				<Parameters>
					<KeepPoseForNumberOfFrames>2</KeepPoseForNumberOfFrames>
					<GravityAssistance></GravityAssistance>
					<AlphaTranslation>0.8</AlphaTranslation>
					<GammaTranslation>0.8</GammaTranslation>
					<AlphaRotation>0.5</AlphaRotation>
					<GammaRotation>0.5</GammaRotation>
					<ContinueLostTrackingWithOrientationSensor>false</ContinueLostTrackingWithOrientationSensor>
				</Parameters>
			</Fuser>

			<SensorSource>
				<SensorID>FeatureTracking1</SensorID>
				<SensorCosID>Patch2</SensorCosID>
				<HandEyeCalibration>
					<TranslationOffset>
						<X>0</X>
						<Y>0</Y>
						<Z>0</Z>
					</TranslationOffset>
					<RotationOffset>
						<X>0</X>
						<Y>0</Y>
						<Z>0</Z>
						<W>1</W>
					</RotationOffset>
				</HandEyeCalibration>
				<COSOffset>
					<TranslationOffset>
						<X>0</X>
						<Y>0</Y>
						<Z>0</Z>
					</TranslationOffset>
					<RotationOffset>
						<X>0</X>
						<Y>0</Y>
						<Z>0</Z>
						<W>1</W>
					</RotationOffset>
				</COSOffset>
			</SensorSource>
		</COS>


	</Connections>
</TrackingData>
